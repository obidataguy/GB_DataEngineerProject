{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2849720-9ee3-4dfd-9c89-7f6d6319cf59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd214fbd-6ca9-4bbc-885a-cd705f679bfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d7770c4-7dcc-4fd3-a9c8-15bfb5a894b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Function to format Text data to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "799a5d8e-a870-4aa7-8a5e-1967b514d37a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reformat_text_data(df, schema):\n",
    "    # from pyspark.sql.functions import substring\n",
    "\n",
    "    column_name = data_raw.columns[0]    # get column name\n",
    "    new_columns = [] # Create an empty list to store the new columns\n",
    "    \n",
    "    for new_col_name, (start, end) in schema.items():\n",
    "        length = end-start+1    # get the length of substring \n",
    "        \n",
    "        # get substrings, apply the new column name and append to the list\n",
    "        new_columns.append(substring(df[column_name], start, length).alias(new_col_name))\n",
    "\n",
    "    # Create the new DataFrame with all the extracted columns\n",
    "    df = df.select(*new_columns)\n",
    "\n",
    "    return df # return a dataframe\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "540686d7-3c21-4696-afe0-7aae7261a25d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Function to Remove leading and trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe942094-012f-4553-a24d-395e56b70de4",
     "showTitle": true,
     "title": "Remove leading trailing spaces"
    }
   },
   "outputs": [],
   "source": [
    "def remove_leading_trailing_spaces(df, except_col):\n",
    "    \n",
    "    # from pyspark.sql.functions import trim, col\n",
    "    columns_to_trim = [col for col in df.columns if col != except_col]\n",
    "\n",
    "     # select all columns but except_col and apply trim function then add except_col\n",
    "    df_trimmed = df.select(*[trim(col(c)).alias(c) for c in columns_to_trim] + [col(except_col)]) \n",
    "    \n",
    "    return df_trimmed     # return the trimmed dataframe  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30d74ff4-2a52-449b-a6b6-3f217d4235cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Function to check for null values in specific datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "780b2a99-70ba-4ed0-a7b7-f18744d8567c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def assert_no_nulls(df, column_name):\n",
    "  # Asserts that a specified column in a DataFrame does not contain any null values.\n",
    "\n",
    "  null_count = df.filter(df[column_name].isNull()).count()\n",
    "  assert null_count == 0, f\"Column '{column_name}' contains {null_count} null values.\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Common Functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
